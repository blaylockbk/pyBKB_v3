{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Brian Blaylock**  \n",
    "**June 24, 2020**  \n",
    "COVID-19 Era\n",
    "\n",
    "# 🏗 Demo: How to download a bunch of HRRR grib2 files.\n",
    "HRRR grib2 files can be downloaded from the [University of Utah's HRRR archive](http://hrrr.chpc.utah.edu/) on the CHPC Pando archive system. You may also get HRRR grib2 files from the [NOAA Operational Model Archive and Distribution System (NOMADS)](https://nomads.ncep.noaa.gov/), but only for the today's and yesterday's runs.\n",
    "\n",
    "---\n",
    "\n",
    "🌐 HRRR Archive Website: http://hrrr.chpc.utah.edu/  \n",
    "📧 Brian Blaylock blaylockbk@gmail.com  \n",
    "✒ Citation this details:\n",
    "> Blaylock B., J. Horel and S. Liston, 2017: Cloud Archiving and Data Mining of High Resolution Rapid Refresh Model Output. Computers and Geosciences. 109, 43-50. https://doi.org/10.1016/j.cageo.2017.08.005\n",
    "\n",
    "---\n",
    "\n",
    "Let's start by importing some modules we will use..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import urllib.request  # Used to download the file\n",
    "import requests        # Used to check if a URL exists\n",
    "import warnings\n",
    "\n",
    "import pandas as pd    # Just used for the date_range function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell is rather lengthy, but it includes the `download_HRRR` function, which we will use to download HRRR files from the University of Utah archive or from NOMADS. Read the document string to understand all the options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reporthook(a, b, c):\n",
    "    \"\"\"\n",
    "    Report download progress in megabytes (prints progress to screen).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a : Chunk number\n",
    "    b : Maximum chunk size\n",
    "    c : Total size of the download\n",
    "    \"\"\"\n",
    "    chunk_progress = a * b / c * 100\n",
    "    total_size_MB =  c / 1000000.\n",
    "    print(f\"\\r Download Progress: {chunk_progress:.2f}% of {total_size_MB:.1f} MB\\r\", end='')\n",
    "\n",
    "def download_HRRR(DATES, fxx=range(0, 1), model='hrrr', field='sfc', \n",
    "                  SOURCE='pando', SAVEDIR='./', dryrun=False):\n",
    "    \"\"\"\n",
    "    Downloads full HRRR grib2 files for a list of dates and forecasts.\n",
    "    \n",
    "    Files are downloaded from the University of Utah HRRR archive (Pando) \n",
    "    or NOAA Operational Model Archive and Distribution System (NOMADS).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    DATES : datetime or list of datetimes\n",
    "        A datetime or list of datetimes that represent the model \n",
    "        initialization time for which you want to download.\n",
    "    fxx : int or list of ints\n",
    "        Forecast lead time or list of forecast lead times to download.\n",
    "        Default only grabs analysis hour (f00), but you might want all\n",
    "        the forecasts hours, in that case, you could set ``fxx=range(0,19)``.\n",
    "    model : {'hrrr', 'hrrrak', 'hrrrX'}\n",
    "        The model type you want to download.\n",
    "        - 'hrrr' HRRR Contiguous United States (operational)\n",
    "        - 'hrrrak' HRRR Alaska. You can also use 'alaska' as an alias.\n",
    "        - 'hrrrX' HRRR *experimental*\n",
    "    field : {'prs', 'sfc', 'nat', 'subh'}\n",
    "        Variable fields you wish to download. \n",
    "        - 'sfc' surface fields\n",
    "        - 'prs' pressure fields\n",
    "        - 'nat' native fields      ('nat' files are not available on Pando)\n",
    "        - 'subh' subhourly fields  ('subh' files are not available on Pando)\n",
    "    SOURCE : {'pando', 'nomads'}\n",
    "        Specify the source from which to download the HRRR files.\n",
    "        - 'pando' downloads HRRR files from University of Utah archive:\n",
    "        http://hrrr.chpc.utah.edu/        \n",
    "        - 'nomads' downloads HRRR files from NCEP NOMADS server:\n",
    "        https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/\n",
    "    SAVEDIR : str\n",
    "        Directory path to save the downloaded HRRR files.\n",
    "    dryrun : bool\n",
    "        If True, instead of downloading the files, it will print out the\n",
    "        files that could be downloaded. This is set to False by default.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Downloads the HRRR files, with filename prepended with the run date\n",
    "    (i.e. `20170101_hrrr.t00z.wrfsfcf00.grib2`)\n",
    "    \"\"\"\n",
    "    \n",
    "    #**************************************************************************\n",
    "    ## Check function input\n",
    "    #**************************************************************************\n",
    "    \n",
    "    # Ping Pando for the first. This *might* prevent a \"bad handshake\" error.\n",
    "    if SOURCE == 'pando':\n",
    "        try:\n",
    "            requests.head('https://pando-rgw01.chpc.utah.edu/')\n",
    "        except:\n",
    "            print('bad handshake...am I able to on?')\n",
    "            pass\n",
    "    \n",
    "    # Force the `SOURCE` and `field` input string to be lower case.\n",
    "    SOURCE = SOURCE.lower()\n",
    "    field = field.lower()\n",
    "\n",
    "    # `DATES` and `fxx` should be a list-like object, but if it doesn't have\n",
    "    # length, (like if the user requests a single date or forecast hour),\n",
    "    # then turn it item into a list-like object.\n",
    "    if not hasattr(DATES, '__len__'): DATES = np.array([DATES])\n",
    "    if not hasattr(fxx, '__len__'): fxx = [fxx]\n",
    "    \n",
    "    # HRRR data on NOMADS is only available for today's and yesterday's runs.\n",
    "    # If any of the DATES are older than yesterday, raise a warning and\n",
    "    # change SOURCE to pando.\n",
    "    if SOURCE == 'nomads':\n",
    "        yesterday = datetime.utcnow() - timedelta(days=1)\n",
    "        yesterday = datetime(yesterday.year, yesterday.month, yesterday.day)\n",
    "        if any(DATES < yesterday):\n",
    "            warnings.warn(\"Changed the SOURCE to 'pando' because one or more of the requested DATES are for more than two days ago.\")\n",
    "            SOURCE = 'pando'\n",
    "    \n",
    "    # The user may set `model='alaska'` as an alias for 'hrrrak'.\n",
    "    if model.lower() == 'alaska': model = 'hrrrak'\n",
    "      \n",
    "    _SOURCE = {'pando', 'nomads'}\n",
    "    assert SOURCE in _SOURCE, f'`SOURCE` must be one of {_SOURCE}'\n",
    "    \n",
    "    # The model type and field depends on the SOURCE the files are downloaded.\n",
    "    if SOURCE == 'pando':\n",
    "        _models = {'hrrr', 'hrrrak', 'hrrrX'}\n",
    "        _fields = {'sfc', 'prs'}\n",
    "    elif SOURCE == 'nomads':\n",
    "        _models = {'hrrr', 'hrrrak'}\n",
    "        _fields = {'sfc', 'prs', 'nat', 'subh'}\n",
    "        \n",
    "    assert model in _models, f'`model` should be set to one of {_models} for `SOURCE={SOURCE}`'\n",
    "    assert field in _fields, f'`field` should be set to one of {_fields} for `SOURCE={SOURCE}`'\n",
    "    \n",
    "    # Make SAVEDIR if path doesn't exist\n",
    "    if not os.path.exists(SAVEDIR):\n",
    "        os.makedirs(SAVEDIR)\n",
    "        print(f'Created directory: {SAVEDIR}')\n",
    "\n",
    "    #**************************************************************************\n",
    "    # Build the URL path for every file we want\n",
    "    #**************************************************************************\n",
    "    # An example URL for a file from Pando is \n",
    "    # https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200624/hrrr.t01z.wrfsfcf17.grib2\n",
    "    # \n",
    "    # An example URL for a file from NOMADS is\n",
    "    # https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200624/conus/hrrr.t00z.wrfsfcf09.grib2\n",
    "    \n",
    "    # `base_url`    : The first part of the URL path\n",
    "    # `file_list`   : A list of full URL paths to each file (one for each \n",
    "    #                 forecast hour requested)\n",
    "    # `file_rename` : A list of names the files will be renamed. It prepends\n",
    "    #                 the original file name with the run date YYYYMMDD.\n",
    "        \n",
    "    if SOURCE == 'pando':\n",
    "        base = f'https://pando-rgw01.chpc.utah.edu/{model}/{field}'\n",
    "        URL_list = [f'{base}/{DATE:%Y%m%d}/{model}.t{DATE:%H}z.wrf{field}f{f:02d}.grib2' for DATE in DATES for f in fxx]\n",
    "    \n",
    "    elif SOURCE == 'nomads':\n",
    "        base = 'https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod'\n",
    "        if model == 'hrrr':\n",
    "            URL_list = [f'{base}/hrrr.{DATE:%Y%m%d}/conus/hrrr.t{DATE:%H}z.wrf{field}f{f:02d}.grib2' for DATE in DATES for f in fxx]\n",
    "        elif model == 'hrrrak':\n",
    "            URL_list = [f'{base}/hrrr.{DATE:%Y%m%d}/alaska/hrrr.t{DATE:%H}z.wrf{field}f{f:02d}.ak.grib2' for DATE in DATES for f in fxx]\n",
    "    \n",
    "    #**************************************************************************\n",
    "    # Ok, so we have a URL and filename for each requested forecast hour.\n",
    "    # Now we need to check if each of those files exist, and if it does,\n",
    "    # we will download that file to the SAVEDIR location.\n",
    "    \n",
    "    for file_URL in URL_list:\n",
    "        # We want to prepend the filename with the run date, YYYYMMDD\n",
    "        if SOURCE == 'pando':\n",
    "            rename = '_'.join(file_URL.split('/')[-2:])\n",
    "        elif SOURCE == 'nomads':\n",
    "            rename = file_URL.split('/')[-3][5:] + '_' + file_URL.split('/')[-1]\n",
    "        \n",
    "        # Check if the URL returns a status code of 200 (meaning the URL is ok)\n",
    "        # Also check that the Content-Length is >1000000 bytes (if it's smaller,\n",
    "        # the file on the server might be incomplete)\n",
    "        head = requests.head(file_URL)\n",
    "        \n",
    "        check_exists = head.ok\n",
    "        check_content = int(head.raw.info()['Content-Length']) > 1000000\n",
    "        \n",
    "        if check_exists and check_content:\n",
    "            # Download the file\n",
    "            if dryrun:\n",
    "                print(f'🌵 Dry Run Success! Would have downloaded {file_URL} as {SAVEDIR+rename}')\n",
    "            else:\n",
    "                urllib.request.urlretrieve(file_URL, SAVEDIR+rename, reporthook)\n",
    "                print(f'✅ Success! Downloaded {file_URL} as {SAVEDIR+rename}')\n",
    "        else:\n",
    "            # The URL request is bad. If status code == 404, the URL does not exist.\n",
    "            print()\n",
    "            print(f'❌ WARNING: Status code {head.status_code}: {head.reason}. Content-Length: {int(head.raw.info()[\"Content-Length\"]):,} bytes')\n",
    "            print(f'❌ Could not download {head.url}')\n",
    "    \n",
    "    print(\"\\nFinished 🍦\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples...\n",
    "Ok, now that you have the `download_HRRR` function, we need to tell it what we want to download.\n",
    "\n",
    "Let's start with a range of dates. We imported the Pandas module just because I really like the `date_range` function.\n",
    "\n",
    "> Note: These dates refer to the model's *initialization* time.\n",
    "\n",
    "We use python's standard `datetime` module to define a start date and end date. Then we use pandas `date_range` function to create a list of dates. We set the freq to be `freq='1H'` becuase we know the HRRR model runs every hour. If you don't want every hour of HRRR data, you could change the frequency (e.g., '3H' would create a list of dates for every 3 hours between the start and end time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2020-04-24 00:00:00', '2020-04-24 01:00:00',\n",
       "               '2020-04-24 02:00:00', '2020-04-24 03:00:00'],\n",
       "              dtype='datetime64[ns]', freq='H')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the start and end date for the HRRR files we want to download\n",
    "sDATE = datetime(2020, 4, 24)\n",
    "eDATE = datetime(2020, 4, 24, 3)\n",
    "\n",
    "# Create a list of datetimes we want to download with Pandas `date_range` function.\n",
    "# The HRRR model is run every hour, so make a list of every hour\n",
    "DATES = pd.date_range(sDATE, eDATE, freq='1H')\n",
    "DATES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other thing you might need to specify is which forecast hours to download. By default, the function will only download the analyis (F00, zero-hour forecast).\n",
    "I like to use `fxx` as my variable for a list of forecast hours, becuase to me it looks like **F00**, **F02**, **F12**, etc.\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "|Code|Output\n",
    "|---|---\n",
    "|`fxx = range(0, 1)`| [0] *this is the function's default*\n",
    "|`fxx = range(0, 19)`| [0, 1, 2, 3, ..., 18]\n",
    "|`fxx = range(0, 19, 3)`| [0, 3, 6, 9, 12, 15, 18]\n",
    "|`fxx = [2, 6, 7, 12]`| [2, 6, 7, 12] *of course, you can make your own list*\n",
    "\n",
    "For this example, lets just download **F00** and **F03**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fxx = range(0, 4, 3)\n",
    "list(fxx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's call the `download_HRRR` function with our specified DATES and forecasts hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success! Downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t00z.wrfsfcf00.grib2 as ./20200424_hrrr.t00z.wrfsfcf00.grib2\n",
      "✅ Success! Downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t00z.wrfsfcf03.grib2 as ./20200424_hrrr.t00z.wrfsfcf03.grib2\n",
      "✅ Success! Downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t01z.wrfsfcf00.grib2 as ./20200424_hrrr.t01z.wrfsfcf00.grib2\n",
      "✅ Success! Downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t01z.wrfsfcf03.grib2 as ./20200424_hrrr.t01z.wrfsfcf03.grib2\n",
      "✅ Success! Downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t02z.wrfsfcf00.grib2 as ./20200424_hrrr.t02z.wrfsfcf00.grib2\n",
      "✅ Success! Downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t02z.wrfsfcf03.grib2 as ./20200424_hrrr.t02z.wrfsfcf03.grib2\n",
      "✅ Success! Downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t03z.wrfsfcf00.grib2 as ./20200424_hrrr.t03z.wrfsfcf00.grib2\n",
      "✅ Success! Downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t03z.wrfsfcf03.grib2 as ./20200424_hrrr.t03z.wrfsfcf03.grib2\n",
      "\n",
      "Finished 🍦\n"
     ]
    }
   ],
   "source": [
    "download_HRRR(DATES, fxx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That just downloaded the HRRR files into the current working directory.\n",
    "\n",
    "We can specify the directory we want to save the files to with the `SAVEDIR` argument. If the directory path doesn't exist, then it will be created. And one more thing...we can do a \"dry run\" of the download, meaning we go through the motions of the function, but skip the actual download. This will show how the download will work. Let's turn on the \"dry run\" for this next test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌵 Dry Run Success! Would have downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t00z.wrfsfcf00.grib2 as ./putInThisDir/20200424_hrrr.t00z.wrfsfcf00.grib2\n",
      "🌵 Dry Run Success! Would have downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t00z.wrfsfcf03.grib2 as ./putInThisDir/20200424_hrrr.t00z.wrfsfcf03.grib2\n",
      "🌵 Dry Run Success! Would have downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t01z.wrfsfcf00.grib2 as ./putInThisDir/20200424_hrrr.t01z.wrfsfcf00.grib2\n",
      "🌵 Dry Run Success! Would have downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t01z.wrfsfcf03.grib2 as ./putInThisDir/20200424_hrrr.t01z.wrfsfcf03.grib2\n",
      "🌵 Dry Run Success! Would have downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t02z.wrfsfcf00.grib2 as ./putInThisDir/20200424_hrrr.t02z.wrfsfcf00.grib2\n",
      "🌵 Dry Run Success! Would have downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t02z.wrfsfcf03.grib2 as ./putInThisDir/20200424_hrrr.t02z.wrfsfcf03.grib2\n",
      "🌵 Dry Run Success! Would have downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t03z.wrfsfcf00.grib2 as ./putInThisDir/20200424_hrrr.t03z.wrfsfcf00.grib2\n",
      "🌵 Dry Run Success! Would have downloaded https://pando-rgw01.chpc.utah.edu/hrrr/sfc/20200424/hrrr.t03z.wrfsfcf03.grib2 as ./putInThisDir/20200424_hrrr.t03z.wrfsfcf03.grib2\n",
      "\n",
      "Finished 🍦\n"
     ]
    }
   ],
   "source": [
    "download_HRRR(DATES, fxx, SAVEDIR='./putInThisDir/', dryrun=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More options\n",
    "\n",
    "The useage above is what most people want from the HRRR archive, but there are three other options we can change.\n",
    "\n",
    "**`model=`**\n",
    "- `'hrrr'`Download the operational HRRR for the contiguous 48 states. *This is the default.*\n",
    "- `'hrrrak'` or `'alaska'` Download the operational HRRR-Alaska domain.\n",
    "- `'hrrrX'` Download the experimental HRRR. **Not available from NOMADS but some analyses are on Pando.**\n",
    "\n",
    "**`field=`**\n",
    "- `'sfc'` Download the surface fields. *This is the default.*\n",
    "- `'prs'` Download the pressure fields\n",
    "- `'nat'` Download the native fields. **Not available on Pando**\n",
    "- `'subh'` Download the subhourly fields. **Not available on Pando**\n",
    "\n",
    "**`SOURCE=`**\n",
    "- `'pando'` Download files from the University of Utah's Pando archive system. *This is the default.*\n",
    "- `'nomads'` Download files from NOMADS server. Files are only available for today and yesterday."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does it look like when we download Alaska grids from NOMADS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/p/home/blaylock/anaconda3/envs/basic/lib/python3.7/site-packages/ipykernel_launcher.py:90: UserWarning: Changed the SOURCE to 'pando' because one or more of the requested DATES are for more than two days ago.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌵 Dry Run Success! Would have downloaded https://pando-rgw01.chpc.utah.edu/hrrrak/sfc/20200424/hrrrak.t00z.wrfsfcf00.grib2 as ./20200424_hrrrak.t00z.wrfsfcf00.grib2\n",
      "🌵 Dry Run Success! Would have downloaded https://pando-rgw01.chpc.utah.edu/hrrrak/sfc/20200424/hrrrak.t00z.wrfsfcf03.grib2 as ./20200424_hrrrak.t00z.wrfsfcf03.grib2\n",
      "\n",
      "❌ WARNING: Status code 404: Not Found. Content-Length: 219 bytes\n",
      "❌ Could not download https://pando-rgw01.chpc.utah.edu/hrrrak/sfc/20200424/hrrrak.t01z.wrfsfcf00.grib2\n",
      "\n",
      "❌ WARNING: Status code 404: Not Found. Content-Length: 219 bytes\n",
      "❌ Could not download https://pando-rgw01.chpc.utah.edu/hrrrak/sfc/20200424/hrrrak.t01z.wrfsfcf03.grib2\n",
      "\n",
      "❌ WARNING: Status code 404: Not Found. Content-Length: 219 bytes\n",
      "❌ Could not download https://pando-rgw01.chpc.utah.edu/hrrrak/sfc/20200424/hrrrak.t02z.wrfsfcf00.grib2\n",
      "\n",
      "❌ WARNING: Status code 404: Not Found. Content-Length: 219 bytes\n",
      "❌ Could not download https://pando-rgw01.chpc.utah.edu/hrrrak/sfc/20200424/hrrrak.t02z.wrfsfcf03.grib2\n",
      "🌵 Dry Run Success! Would have downloaded https://pando-rgw01.chpc.utah.edu/hrrrak/sfc/20200424/hrrrak.t03z.wrfsfcf00.grib2 as ./20200424_hrrrak.t03z.wrfsfcf00.grib2\n",
      "🌵 Dry Run Success! Would have downloaded https://pando-rgw01.chpc.utah.edu/hrrrak/sfc/20200424/hrrrak.t03z.wrfsfcf03.grib2 as ./20200424_hrrrak.t03z.wrfsfcf03.grib2\n",
      "\n",
      "Finished 🍦\n"
     ]
    }
   ],
   "source": [
    "download_HRRR(DATES, fxx, model='alaska', SOURCE='nomads', dryrun=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things about that above example...\n",
    "1. There is a **UserWarning** that says the `SOURCE` was changed to download from 'pando'. That is becuase the run DATES we requested are older than two days and are not available on NOMADS.\n",
    "2. A printed **WARNING** tells us the requested URL could not be found for a few of our requested files. That is becuase the HRRR-Alaska model only runs at 00z, 03z, 06z, 12z, 15z, 18z, and 21z. It does not run hourly like the HRRR model. When retrieveing Alaska files, you should set your date_range with a 3-hour interval (e.g. `DATES = pd.date_range(sDATE, eDATE, freq='3H')`).\n",
    "3. Remember that we ran this with `dryrun=True`, meaning we didn't actually download the files, but it told us what it would have downloaded and where it would have saved the file.\n",
    "\n",
    "> If you get `WARNING: Status code 404`, you might want to check that the file exists. One way to do that on Pando is to use the [interactive web download interface](home.chpc.utah.edu/~u0553130/Brian_Blaylock/cgi-bin/hrrr_download.cgi) and check if the file you are trying to download is available. You might want to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's request the F00-F19 forecasts from a single run from yesterday and do a \"dryrun\" to download files from NOMADS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌵 Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200624/conus/hrrr.t02z.wrfsfcf00.grib2 as ./20200624_hrrr.t02z.wrfsfcf00.grib2\n",
      "🌵 Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200624/conus/hrrr.t02z.wrfsfcf01.grib2 as ./20200624_hrrr.t02z.wrfsfcf01.grib2\n",
      "🌵 Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200624/conus/hrrr.t02z.wrfsfcf02.grib2 as ./20200624_hrrr.t02z.wrfsfcf02.grib2\n",
      "🌵 Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200624/conus/hrrr.t02z.wrfsfcf03.grib2 as ./20200624_hrrr.t02z.wrfsfcf03.grib2\n",
      "🌵 Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200624/conus/hrrr.t02z.wrfsfcf04.grib2 as ./20200624_hrrr.t02z.wrfsfcf04.grib2\n",
      "🌵 Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200624/conus/hrrr.t02z.wrfsfcf05.grib2 as ./20200624_hrrr.t02z.wrfsfcf05.grib2\n",
      "🌵 Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200624/conus/hrrr.t02z.wrfsfcf06.grib2 as ./20200624_hrrr.t02z.wrfsfcf06.grib2\n",
      "🌵 Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200624/conus/hrrr.t02z.wrfsfcf07.grib2 as ./20200624_hrrr.t02z.wrfsfcf07.grib2\n",
      "🌵 Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200624/conus/hrrr.t02z.wrfsfcf08.grib2 as ./20200624_hrrr.t02z.wrfsfcf08.grib2\n",
      "🌵 Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200624/conus/hrrr.t02z.wrfsfcf09.grib2 as ./20200624_hrrr.t02z.wrfsfcf09.grib2\n",
      "🌵 Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200624/conus/hrrr.t02z.wrfsfcf10.grib2 as ./20200624_hrrr.t02z.wrfsfcf10.grib2\n",
      "🌵 Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200624/conus/hrrr.t02z.wrfsfcf11.grib2 as ./20200624_hrrr.t02z.wrfsfcf11.grib2\n",
      "🌵 Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200624/conus/hrrr.t02z.wrfsfcf12.grib2 as ./20200624_hrrr.t02z.wrfsfcf12.grib2\n",
      "🌵 Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200624/conus/hrrr.t02z.wrfsfcf13.grib2 as ./20200624_hrrr.t02z.wrfsfcf13.grib2\n",
      "🌵 Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200624/conus/hrrr.t02z.wrfsfcf14.grib2 as ./20200624_hrrr.t02z.wrfsfcf14.grib2\n",
      "🌵 Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200624/conus/hrrr.t02z.wrfsfcf15.grib2 as ./20200624_hrrr.t02z.wrfsfcf15.grib2\n",
      "🌵 Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200624/conus/hrrr.t02z.wrfsfcf16.grib2 as ./20200624_hrrr.t02z.wrfsfcf16.grib2\n",
      "🌵 Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200624/conus/hrrr.t02z.wrfsfcf17.grib2 as ./20200624_hrrr.t02z.wrfsfcf17.grib2\n",
      "🌵 Dry Run Success! Would have downloaded https://nomads.ncep.noaa.gov/pub/data/nccf/com/hrrr/prod/hrrr.20200624/conus/hrrr.t02z.wrfsfcf18.grib2 as ./20200624_hrrr.t02z.wrfsfcf18.grib2\n",
      "\n",
      "Finished 🍦\n"
     ]
    }
   ],
   "source": [
    "yesterday = datetime.utcnow() - timedelta(days=1)\n",
    "\n",
    "download_HRRR(yesterday, fxx=range(0,19), model='hrrr', SOURCE='nomads', dryrun=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*And there you have it, a fancy function to help you download a bunch of HRRR grib2 files from the University of Utah's HRRR archive on Pando and NOMADS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
